{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession,SQLContext\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import col, split\n",
    "from pyspark.ml.feature import RegexTokenizer, CountVectorizer, PCA ,StopWordsRemover,StringIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import RandomForestClassifier,LogisticRegression,NaiveBayes,LinearSVC\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.sql.types import DoubleType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loading(path):\n",
    "    # creating spark session\n",
    "    spark = SparkSession.builder.appName(\"spooky\").master('local[*]').config(\"spark.executor.memory\", '6G').config(\"spark.driver.memory\", '6G').config(\"spark.driver.maxResultSize\", '7G').getOrCreate()\n",
    "    # loading csv into dataframe\n",
    "    df = spark.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load(path)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(df):\n",
    "    # droping the used column\n",
    "    df=df.drop('_c2').drop('_c3').drop('_c4')\n",
    "    # changing the name of the colmun\n",
    "    df = df.selectExpr(\"v1 as class\", \"v2 as text\")\n",
    "    # removing regrex from label column \n",
    "    df=df.withColumn('String_Label', F.regexp_replace('class', '\\\\W', ''))\n",
    "    # removing the null value\n",
    "    df=df.filter(df.text != '')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pyspark_lib(df):\n",
    "    # regex tokenizer\n",
    "    regexTokenizer = RegexTokenizer(inputCol=\"text\", outputCol=\"tokenized\", pattern=\"\\\\W\")\n",
    "    resultantdf=regexTokenizer.transform(df)\n",
    "    # removal of stop word\n",
    "    stopwordsRemover = StopWordsRemover(inputCol=\"tokenized\", outputCol=\"filtered\")\n",
    "    resultantdf=stopwordsRemover.transform(resultantdf)\n",
    "    #count vectorizer implemetation\n",
    "    cv = CountVectorizer(inputCol=\"filtered\", outputCol=\"features\")\n",
    "    model=cv.fit(resultantdf)\n",
    "    result=model.transform(resultantdf)\n",
    "    result=result.drop('tokenized').drop('filtered')\n",
    "    result=result.drop('class')\n",
    "    #converting the String label to integer label\n",
    "    indexer = StringIndexer(inputCol=\"String_Label\", outputCol=\"label\")\n",
    "    indexed = indexer.fit(result).transform(result)\n",
    "    indexed=indexed.drop('String_Label')\n",
    "    # Splitting of data set\n",
    "    (trainingData, testData) = indexed.randomSplit([0.7, 0.3], seed = 100)\n",
    "    return trainingData,testData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_implementation(model,trainingData,testData):\n",
    "    if (model==1):\n",
    "        # Using RandomForestClassifier to train the model\n",
    "        rf = RandomForestClassifier(labelCol=\"label\",featuresCol=\"features\",numTrees = 30,maxDepth = 20)\n",
    "        # Train model with Training Data\n",
    "        rfModel = rf.fit(trainingData)\n",
    "        # Prediction\n",
    "        predictions = rfModel.transform(testData)\n",
    "        evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "        evaluator.evaluate(predictions)\n",
    "    if(model==2):\n",
    "        # Using Logistic Regression to train the model\n",
    "        lr=LogisticRegression(labelCol=\"label\",featuresCol=\"features\")\n",
    "        # train the model \n",
    "        lrmodel=lr.fit(trainingData)\n",
    "        # Prediction\n",
    "        predictions = rfModel.transform(testData)\n",
    "        evaluatorlr = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "        evaluatorlr.evaluate(predictions)\n",
    "    if(model==3):\n",
    "        # using Naive Bayes to train the model\n",
    "        nb = NaiveBayes(smoothing=1)\n",
    "        # train the model\n",
    "        model = nb.fit(trainingData)\n",
    "        # prediction\n",
    "        predictions= model.transform(testData)\n",
    "        evaluatorlr = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "        evaluatorlr.evaluate(predictions)\n",
    "    if(model==4):\n",
    "        # using svm\n",
    "        lsvc = LinearSVC(maxIter=10, regParam=0.1)\n",
    "        # Fit the model\n",
    "        lsvcModel = lsvc.fit(trainingData)\n",
    "        predictions = lsvcModel.transform(testData)\n",
    "        evaluatorsvm = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "        evaluatorsvm.evaluate(predictions)\n",
    "    if(model==5):\n",
    "        # Trains a k-means model.\n",
    "        kmeans = KMeans().setK(2).setSeed(1)\n",
    "        kmodel = kmeans.fit(trainingData)\n",
    "        predictions=kmodel.transform(testData)\n",
    "        # converting column datatype from integer to double\n",
    "        predictions = predictions.withColumn(\"prediction\", predictions[\"prediction\"].cast(DoubleType()))\n",
    "        evaluatorsvm = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "        evaluatorsvm.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=('Trains the model and outputs predictions.'),\n",
    "        add_help='How to use', prog='pyspark_implementation.py <args>')\n",
    "\n",
    "    # Required arguments\n",
    "    parser.add_argument(\"--data_path\", required=True,\n",
    "                        help=(\"Provide the path to the data folder\"))\n",
    "    parser.add_argument('--model', type=str, choices=['random_forest', 'logistic_regression', 'navies bayes', 'svm','kmean'], \n",
    "    default='unet', help = 'model to use for spam classification')\n",
    "\n",
    "    args = vars(parser.parse_args())\n",
    "\n",
    "    # Getting the names of the training / testing files.\n",
    "    path=args['data_path']\n",
    "    df=loading(path)\n",
    "    df=preprocessing(df\n",
    "    trainingData,testData=pyspark_lib(df)\n",
    "    model_implementation(model,trainingData,testData)\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
